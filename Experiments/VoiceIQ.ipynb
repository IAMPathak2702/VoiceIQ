{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d7d6dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage , AIMessage\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c9aec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "039c5866",
   "metadata": {},
   "outputs": [],
   "source": [
    "Instructions = [\n",
    "        HumanMessage(content=\"Ask me a question\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07c92a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    \n",
    "    (\"system\", \"\"\"You are an AI interviewer tasked with \n",
    "    asking questions about generative AI. Ask questions about various aspects of generative AI,\n",
    "    ensuring to cover fundamental concepts, applications, challenges, and future trends.\n",
    "    always respond with question\"\"\"),\n",
    "\n",
    "    (\"user\", \"Ask me a Question {answer}\")])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "139cb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator_chain = question_prompt_template | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5bda3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are some common types of generative AI models used in practice?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_generator_chain.invoke({\"yes\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "885dd932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question():    \n",
    "    question_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    \n",
    "    (\"system\", \"\"\"You are an AI interviewer tasked with \n",
    "    asking questions about generative AI. Ask questions about various aspects of generative AI,\n",
    "    ensuring to cover fundamental concepts, applications, challenges, and future trends.\n",
    "    always respond with question\"\"\"),\n",
    "\n",
    "    (\"user\", \"{answer}\")])\n",
    "    \n",
    "    question_generator_chain = question_prompt_template | llm | parser\n",
    "    \n",
    "    question = question_generator_chain.invoke({\"Ask me a diffrent Question\"})\n",
    "    \n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64e73a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_question(question:str ,answer:str):\n",
    "    \n",
    "    question_prompt_template = ChatPromptTemplate.from_messages([\n",
    "         (\"system\", \"\"\"You are an professional answer checker that checks {answers} for {question} and check weather the\n",
    "         answer is correct or not! if answer is less than 80% match then consider it as incorrect\"\"\"),\n",
    "         \n",
    "         (\"user\", \"{answer}\")])\n",
    "        \n",
    "    question_answer_chain = question_prompt_template | llm | parser\n",
    "    \n",
    "    answer = question_answer_chain.invoke({\"question\": ask_question(), \"answer\":\"\"})\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf233f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are some common types of generative AI models used in practice, and how do they differ from each other in terms of their approach to generating content?\n",
      "Your answer: yahu\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ChatPromptTemplate' object has no attribute 'bind_chain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m user_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour answer: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_answer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(analysis)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m analysis\u001b[38;5;241m.\u001b[39mlower():\n",
      "Cell \u001b[1;32mIn[56], line 9\u001b[0m, in \u001b[0;36manalyze_answer\u001b[1;34m(question, answer)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_answer\u001b[39m(question, answer):\n\u001b[0;32m      2\u001b[0m     question_prompt_template \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([\n\u001b[0;32m      3\u001b[0m         SystemMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[0;32m      4\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a professional answer checker that checks \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m for the question: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and determines whether the answer is correct or not. If the answer is less than 80% correct, consider it incorrect.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      5\u001b[0m         ),\n\u001b[0;32m      6\u001b[0m         HumanMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{answer}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      7\u001b[0m     ])\n\u001b[1;32m----> 9\u001b[0m     question_answer_chain \u001b[38;5;241m=\u001b[39m \u001b[43mquestion_prompt_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_chain\u001b[49m(llm, parser)\n\u001b[0;32m     11\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m question_answer_chain\u001b[38;5;241m.\u001b[39mpredict(answer\u001b[38;5;241m=\u001b[39manswer)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ChatPromptTemplate' object has no attribute 'bind_chain'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = ask_question()\n",
    "    print(f\"Question: {question}\")\n",
    "\n",
    "    user_answer = input(\"Your answer: \")\n",
    "    analysis = analyze_answer(question,user_answer)\n",
    "    print(analysis)\n",
    "\n",
    "    if \"correct\" in analysis.lower():\n",
    "        continue\n",
    "    else:\n",
    "        explanation = llm.generate_prompt(\n",
    "            [\n",
    "                SystemMessage(content=\"You are an AI assistant explaining generative AI concepts.\"),\n",
    "                HumanMessage(content=f\"Please explain the correct answer to the question: {question}\"),\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "        ).content\n",
    "        print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b79368c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vpved\\anaconda3\\envs\\GAI\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'LLMChain' has no attribute 'from_llm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize the OpenAI API\u001b[39;00m\n\u001b[0;32m     12\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m(llm)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask_question\u001b[39m():\n\u001b[0;32m     16\u001b[0m     question_prompt_template \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([\n\u001b[0;32m     17\u001b[0m         SystemMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"You are an AI interviewer tasked with asking questions about generative AI. Ask questions about various aspects of generative AI, ensuring to cover fundamental concepts, applications, challenges, and future trends. Always respond with a question.\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         ),\n\u001b[0;32m     20\u001b[0m         HumanMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{answer}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     21\u001b[0m     ])\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'LLMChain' has no attribute 'from_llm'"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize the OpenAI API\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "parser = LLMChain.from_llm(llm)\n",
    "\n",
    "def ask_question():\n",
    "    question_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"\"\"You are an AI interviewer tasked with asking questions about generative AI. Ask questions about various aspects of generative AI, ensuring to cover fundamental concepts, applications, challenges, and future trends. Always respond with a question.\"\"\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{answer}\"),\n",
    "    ])\n",
    "\n",
    "    question_generator_chain = question_prompt_template.bind_chain(llm, parser)\n",
    "\n",
    "    question = question_generator_chain.predict(answer=\"Ask me a different question\")\n",
    "\n",
    "    return question\n",
    "\n",
    "def analyze_answer(question, answer):\n",
    "    question_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            f\"\"\"You are a professional answer checker that checks {{answer}} for the question: \"{question}\" and determines whether the answer is correct or not. If the answer is less than 80% correct, consider it incorrect.\"\"\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{answer}\"),\n",
    "    ])\n",
    "\n",
    "    question_answer_chain = question_prompt_template.bind_chain(llm, parser)\n",
    "\n",
    "    analysis = question_answer_chain.predict(answer=answer)\n",
    "\n",
    "    return analysis\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    question = ask_question()\n",
    "    print(f\"Question: {question}\")\n",
    "\n",
    "    user_answer = input(\"Your answer: \")\n",
    "    analysis = analyze_answer(question, user_answer)\n",
    "    print(analysis)\n",
    "\n",
    "    if \"correct\" in analysis.lower():\n",
    "        continue\n",
    "    else:\n",
    "        explanation = llm.generate_prompt(\n",
    "            [\n",
    "                SystemMessage(content=\"You are an AI assistant explaining generative AI concepts.\"),\n",
    "                HumanMessage(content=f\"Please explain the correct answer to the question: {question}\"),\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "        ).content\n",
    "        print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac68f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
